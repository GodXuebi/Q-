{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Miniconda\\Miniconda1\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "5000\n",
      "Loss: 269.322\n",
      "Epsilon: 0.951717130256\n",
      "10000\n",
      "Loss: 204.216\n",
      "Epsilon: 0.905789043856\n",
      "15000\n",
      "Loss: 5.75059\n",
      "Epsilon: 0.862100896661\n",
      "20000\n",
      "Loss: 0.0283346\n",
      "Epsilon: 0.820543445547\n",
      "25000\n",
      "Loss: 0.00182071\n",
      "Epsilon: 0.781012775241\n",
      "30000\n",
      "Loss: 0.00436149\n",
      "Epsilon: 0.743410038475\n",
      "30000\n",
      "Loss: 0.000393029\n",
      "Epsilon: 0.743410038475\n",
      "35000\n",
      "Loss: 0.000346293\n",
      "Epsilon: 0.707641208822\n",
      "40000\n",
      "Loss: 2.55276e-05\n",
      "Epsilon: 0.673616845575\n",
      "45000\n",
      "Loss: 1.85636e-05\n",
      "Epsilon: 0.641251870106\n",
      "50000\n",
      "Loss: 0.000226175\n",
      "Epsilon: 0.610465353116\n",
      "55000\n",
      "Loss: 2.00213e-05\n",
      "Epsilon: 0.581180312277\n",
      "60000\n",
      "Loss: 5.94222e-07\n",
      "Epsilon: 0.553323519733\n",
      "65000\n",
      "Loss: 1.79872e-07\n",
      "Epsilon: 0.526825318993\n",
      "70000\n",
      "Loss: 2.19772e-07\n",
      "Epsilon: 0.501619450753\n",
      "75000\n",
      "Loss: 5.34892e-07\n",
      "Epsilon: 0.477642887214\n",
      "80000\n",
      "Loss: 1.96509e-08\n",
      "Epsilon: 0.454835674476\n",
      "80000\n",
      "Loss: 9.76239e-09\n",
      "Epsilon: 0.454835674476\n",
      "85000\n",
      "Loss: 3.90331e-09\n",
      "Epsilon: 0.433140782629\n",
      "90000\n",
      "Loss: 1.64691e-09\n",
      "Epsilon: 0.412503963143\n",
      "95000\n",
      "Loss: 1.06787e-08\n",
      "Epsilon: 0.39287361322\n",
      "100000\n",
      "Loss: 2.31281e-09\n",
      "Epsilon: 0.37420064676\n",
      "105000\n",
      "Loss: 3.67544e-08\n",
      "Epsilon: 0.35643837162\n",
      "110000\n",
      "Loss: 8.13604e-10\n",
      "Epsilon: 0.339542372861\n",
      "110000\n",
      "Loss: 5.22171e-10\n",
      "Epsilon: 0.339542372861\n",
      "115000\n",
      "Loss: 1.13747e-09\n",
      "Epsilon: 0.323470401685\n",
      "115000\n",
      "Loss: 1.37043e-09\n",
      "Epsilon: 0.323470401685\n",
      "120000\n",
      "Loss: 1.07248e-09\n",
      "Epsilon: 0.308182269793\n",
      "125000\n",
      "Loss: 8.10542e-10\n",
      "Epsilon: 0.293639748892\n",
      "130000\n",
      "Loss: 2.88577e-09\n",
      "Epsilon: 0.279806475104\n",
      "135000\n",
      "Loss: 1.0823e-09\n",
      "Epsilon: 0.266647858039\n",
      "140000\n",
      "Loss: 1.28639e-07\n",
      "Epsilon: 0.254130994302\n",
      "145000\n",
      "Loss: 1.44173e-09\n",
      "Epsilon: 0.242224585213\n",
      "150000\n",
      "Loss: 1.22588e-09\n",
      "Epsilon: 0.230898858547\n",
      "155000\n",
      "Loss: 1.38886e-09\n",
      "Epsilon: 0.220125494088\n",
      "160000\n",
      "Loss: 3.02789e-09\n",
      "Epsilon: 0.209877552815\n",
      "165000\n",
      "Loss: 4.39104e-10\n",
      "Epsilon: 0.200129409535\n",
      "170000\n",
      "Loss: 9.65034e-10\n",
      "Epsilon: 0.190856688812\n",
      "175000\n",
      "Loss: 7.60701e-10\n",
      "Epsilon: 0.182036204016\n",
      "175000\n",
      "Loss: 7.10255e-10\n",
      "Epsilon: 0.182036204016\n",
      "180000\n",
      "Loss: 9.80314e-10\n",
      "Epsilon: 0.173645899339\n",
      "180000\n",
      "Loss: 6.36404e-10\n",
      "Epsilon: 0.173645899339\n",
      "180000\n",
      "Loss: 9.61518e-10\n",
      "Epsilon: 0.173645899339\n",
      "185000\n",
      "Loss: 5.89595e-10\n",
      "Epsilon: 0.16566479465\n",
      "190000\n",
      "Loss: 6.75815e-10\n",
      "Epsilon: 0.15807293303\n",
      "195000\n",
      "Loss: 9.79374e-10\n",
      "Epsilon: 0.150851330871\n",
      "200000\n",
      "Loss: 2.44957e-10\n",
      "Epsilon: 0.143981930404\n",
      "****************** Agent is in the State 0 **********************\n",
      "Step is  1 ****************** Agent is in the State 1 **********************\n",
      "Step is  2 ****************** Agent is in the State 16 **********************\n",
      "Step is  3 ****************** Agent is in the State 31 **********************\n",
      "Step is  4 ****************** Agent is in the State 46 **********************\n",
      "Step is  5 ****************** Agent is in the State 47 **********************\n",
      "Step is  6 ****************** Agent is in the State 48 **********************\n",
      "Step is  7 ****************** Agent is in the State 49 **********************\n",
      "Step is  8 ****************** Agent is in the State 64 **********************\n",
      "Step is  9 ****************** Agent is in the State 79 **********************\n",
      "Step is  10 ****************** Agent is in the State 94 **********************\n",
      "Step is  11 ****************** Agent is in the State 109 **********************\n",
      "Step is  12 ****************** Agent is in the State 110 **********************\n",
      "Step is  13 ****************** Agent is in the State 125 **********************\n",
      "Step is  14 ****************** Agent is in the State 126 **********************\n",
      "Step is  15 ****************** Agent is in the State 127 **********************\n",
      "Step is  16 ****************** Agent is in the State 142 **********************\n",
      "Step is  17 ****************** Agent is in the State 157 **********************\n",
      "Step is  18 ****************** Agent is in the State 158 **********************\n",
      "Step is  19 ****************** Agent is in the State 159 **********************\n",
      "Step is  20 ****************** Agent is in the State 160 **********************\n",
      "Step is  21 ****************** Agent is in the State 161 **********************\n",
      "Step is  22 ****************** Agent is in the State 176 **********************\n",
      "Step is  23 ****************** Agent is in the State 177 **********************\n",
      "Step is  24 ****************** Agent is in the State 192 **********************\n",
      "Step is  25 ****************** Agent is in the State 207 **********************\n",
      "Step is  26 ****************** Agent is in the State 208 **********************\n",
      "Step is  27 ****************** Agent is in the State 209 **********************\n",
      "Step is  28 ****************** Agent is in the State 224 **********************\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt5\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "class Q_Network:\n",
    "    N_Mesh=15\n",
    "    State_Number=N_Mesh*N_Mesh;\n",
    "    Action_Number=4;\n",
    "    Action_List=np.identity(Action_Number)\n",
    "    State_List=np.identity(State_Number)\n",
    "    Epsilon=0\n",
    "    Epsilon_Begin=1\n",
    "    Epsilon_Final=0.01\n",
    "    decay_rate=0.00001\n",
    "    Step=0\n",
    "    Batch_Number=30\n",
    "    #Explore = 100000.\n",
    "    Observe=1000#The number of steps of observation before the beginning of the training\n",
    "    Store_Memory = deque()\n",
    "    Cost_History=[]\n",
    "    Memory_Size = 5000\n",
    "    #Obstacle=np.random.randint(State_Number-2,size=100)\n",
    "    Obstacle=[3,5,16,21,23,27,29,31,33,34,56,57,66,67,78,81,86]\n",
    "    Play_Done=None\n",
    "    def __init__(self,Learning_Rate=0.001,Gamma=0.9,Memory_Size=5000):\n",
    "        self.Learning_Rate=Learning_Rate\n",
    "        self.Gamma=Gamma\n",
    "        self.Memory_Size=Memory_Size\n",
    "        self.New_Network()\n",
    "        self.session=tf.InteractiveSession()\n",
    "        self.session.run(tf.initialize_all_variables())\n",
    "     #***************************************************************\n",
    "    def New_Network(self):\n",
    "         #Construct a neural network\n",
    "        self.State_Input=tf.placeholder(tf.float32,[None,self.State_Number])#Input_Number=100, M=Batch*100\n",
    "        self.Action_Input=tf.placeholder(tf.float32,[None,self.Action_Number])#Input_Number=4, M=Batch*4\n",
    "        self.Q_Target_OI=tf.placeholder(dtype=tf.float32,shape=[None])\n",
    "        #Layer1\n",
    "        Layer1_Number=200\n",
    "        w1=tf.Variable(tf.random_normal([self.State_Number,Layer1_Number]))\n",
    "        b1=tf.Variable(tf.zeros([1,Layer1_Number])+0.1)\n",
    "        l1=tf.nn.relu(tf.matmul(self.State_Input,w1)+b1) #b1 use the propagation mechanism\n",
    "#         #Layer2\n",
    "#         Layer2_Number=10\n",
    "#         w2=tf.Variable(tf.random_normal([Layer1_Number,Layer2_Number]))\n",
    "#         b2=tf.Variable(tf.zeros([1,Layer2_Number]))\n",
    "#         l2=tf.nn.relu(tf.matmul(l1,w2)+b2)\n",
    "        #Layer3\n",
    "        Layer3_Number=self.Action_Number\n",
    "        w3=tf.Variable(tf.random_normal([Layer1_Number,Layer3_Number]))\n",
    "        b3=tf.Variable(tf.zeros([1,Layer3_Number]))\n",
    "        self.l3=tf.matmul(l1,w3)+b3#At this time, not yet use the active action, l3 is the output matrice, M=Batch*4\n",
    "        #self.l3=tf.tanh(tf.matmul(l1,w2)+b2)\n",
    "\n",
    "        self.Q_Value=tf.reduce_sum(tf.multiply(self.l3,self.Action_Input),reduction_indices=1)\n",
    "        self.Loss=tf.reduce_mean(tf.square(self.Q_Value-self.Q_Target_OI))\n",
    "        #self.Loss=tf.reduce_mean(tf.square(self.Q_Value-tf.tanh(self.Q_Target_OI)))\n",
    "        self.Optimizer=tf.train.GradientDescentOptimizer(self.Learning_Rate).minimize(self.Loss)\n",
    "        #Network's Output is like this form [19,20,297,30], argmax is 2, then choose Action2  \n",
    "        self.Predict=tf.argmax(self.l3,1)\n",
    "     #***************************************************************\n",
    "    def Select_Action(self,State_Index):\n",
    "        Current_State=self.State_List[State_Index:State_Index+1] #By example, Action_Index is 2, Current_State=[0,0,1,0， ，]\n",
    "        if np.random.uniform()<self.Epsilon:\n",
    "            Choose_Action_Index=np.random.randint(0,self.Action_Number)\n",
    "        else:\n",
    "            Action_QValue_Output=self.session.run(self.l3,feed_dict={self.State_Input:Current_State})\n",
    "            Choose_Action_Index=np.argmax(Action_QValue_Output)\n",
    "        #The first inegality means the train has beginned\n",
    "        #The second inegality means Epsilon has not yet decreased to final value\n",
    "        self.Epsilon=self.Epsilon_Final+(self.Epsilon_Begin-self.Epsilon_Final)*np.exp(-self.decay_rate*self.Step)\n",
    "#         if(self.Step%1000==0):\n",
    "#             print(\"epsilon \",self.Epsilon)\n",
    "        return Choose_Action_Index\n",
    "     #***************************************************************\n",
    "    def Next_State(self,Action_Index,State_Index):\n",
    "        State=State_Index+1\n",
    "        done=False\n",
    "        #Execute left motion\n",
    "        if(Action_Index==0):\n",
    "            if(State%self.N_Mesh==1):\n",
    "                State=State\n",
    "                R=-1000\n",
    "            elif((State+1) in self.Obstacle):#When encounter obstables, we stay \n",
    "                State=State\n",
    "                R=-1000\n",
    "            else:\n",
    "                State=State-1\n",
    "                R=0\n",
    "        elif(Action_Index==1):\n",
    "            if(State==self.State_Number-1):\n",
    "                State=State+1\n",
    "                done=True\n",
    "                R=100\n",
    "            elif(State%self.N_Mesh==0):\n",
    "                State=State\n",
    "                R=-100\n",
    "            elif((State+1) in self.Obstacle):\n",
    "                State=State\n",
    "                R=-100\n",
    "            else:\n",
    "                State=State+1\n",
    "                R=0.02\n",
    "        elif(Action_Index==2):\n",
    "            if(State==self.State_Number-self.N_Mesh):\n",
    "                State=State+self.N_Mesh\n",
    "                done=True\n",
    "                R=100\n",
    "            elif(self.State_Number+1-self.N_Mesh<=State<=self.State_Number-1):\n",
    "                State=State\n",
    "                R=-100\n",
    "            elif((State+self.N_Mesh) in self.Obstacle):\n",
    "                State=State+self.N_Mesh\n",
    "                R=-100\n",
    "            else:\n",
    "                State=State+self.N_Mesh\n",
    "                R=0.02\n",
    "        else:\n",
    "            if(0<=State<=self.N_Mesh):\n",
    "                State=State\n",
    "                R=-100\n",
    "            elif((State-self.N_Mesh) in self.Obstacle):\n",
    "                State=State-self.N_Mesh\n",
    "                R=-100\n",
    "            else:\n",
    "                R=0\n",
    "                State=State-self.N_Mesh\n",
    "        return State-1,R,done\n",
    "    \n",
    "    def Save_Memory(self,CURRENT_STATET,CHOOSE_ACTION,NEXT_STATE,REWARD,DONE):\n",
    "        Memory_Current_State=self.State_List[CURRENT_STATET:CURRENT_STATET+1]\n",
    "        Memory_Choose_Action=self.Action_List[CHOOSE_ACTION:CHOOSE_ACTION+1]\n",
    "        Memory_Next_State=self.State_List[NEXT_STATE:NEXT_STATE+1]\n",
    "        self.Store_Memory.append((Memory_Current_State,Memory_Choose_Action,Memory_Next_State,REWARD,DONE))\n",
    "        if len(self.Store_Memory)>self.Memory_Size:\n",
    "            self.Store_Memory.popleft()\n",
    "    \n",
    "     #***************************************************************\n",
    "    def Experience_Replay(self):\n",
    "        Batch=self.Batch_Number\n",
    "        MiniBatch=random.sample(self.Store_Memory,Batch)\n",
    "        Batch_Current_State = None\n",
    "        Batch_Execute_Action = None\n",
    "        Batch_Reward = None\n",
    "        Batch_Next_State = None\n",
    "        Batch_Done = None\n",
    "        \n",
    "        for Index in range(Batch):\n",
    "            if Batch_Current_State is None:\n",
    "                Batch_Current_State=MiniBatch[Index][0]\n",
    "            elif Batch_Current_State is not None:\n",
    "                Batch_Current_State=np.vstack((Batch_Current_State, MiniBatch[Index][0]))\n",
    "            #---------------------------------------------------------------------------------------   \n",
    "            #print(\"BEA\",MiniBatch[Index][1].shape)\n",
    "            if Batch_Execute_Action is None:\n",
    "                Batch_Execute_Action=MiniBatch[Index][1]\n",
    "            elif Batch_Execute_Action is not None:\n",
    "                Batch_Execute_Action=np.vstack((Batch_Execute_Action,MiniBatch[Index][1]))\n",
    "           #---------------------------------------------------------------------------------------           \n",
    "            if Batch_Reward is None:\n",
    "                Batch_Reward=MiniBatch[Index][3]\n",
    "            elif Batch_Reward is not None:\n",
    "                Batch_Reward=np.vstack((Batch_Reward,MiniBatch[Index][3]))\n",
    "            #---------------------------------------------------------------------------------------  \n",
    "            if Batch_Next_State is None:\n",
    "                Batch_Next_State=MiniBatch[Index][2]\n",
    "            elif Batch_Next_State is not None:\n",
    "                Batch_Next_State=np.vstack((Batch_Next_State,MiniBatch[Index][2]))\n",
    "            #---------------------------------------------------------------------------------------      \n",
    "            if Batch_Done is None:\n",
    "                Batch_Done=MiniBatch[Index][4]\n",
    "            elif Batch_Done is not None:\n",
    "                Batch_Done=np.vstack((Batch_Done,MiniBatch[Index][4]))\n",
    "            #Calculate the Q Value of the next State \n",
    "        Q_Next=self.session.run(self.l3,feed_dict={self.State_Input:Batch_Next_State})\n",
    "        Q_Target=[]\n",
    "        for i in range(Batch):\n",
    "            Each_Reward=Batch_Reward[i][0]#This is a 2D array because of the vstack\n",
    "            #Calculate the Target-Q-Value of each element in the Batch\n",
    "            Each_QValue=Each_Reward+self.Gamma*np.max(Q_Next[i]) #The network ouput has its own []\n",
    "            if Each_Reward<0:\n",
    "                Q_Target.append(Each_Reward)\n",
    "            else:\n",
    "                Q_Target.append(Each_QValue)\n",
    "        #print(self.session.run(self.Q_Value,feed_dict={self.Q_Target_OI:Q_Target}))\n",
    "        _,Cost,Rew=self.session.run([self.Q_Value,self.Loss,self.Optimizer],feed_dict={self.State_Input:Batch_Current_State,\n",
    "                                                                                        self.Action_Input:Batch_Execute_Action,\n",
    "                                                                                        self.Q_Target_OI: Q_Target})\n",
    "        self.Cost_History.append(Cost)\n",
    "        if self.Step%5000==0:\n",
    "            print(self.Step)  \n",
    "            print(\"Loss:\", Cost)  \n",
    "            print(\"Epsilon:\", self.Epsilon)   \n",
    "    #**************************************************************\n",
    "    def Train(self):\n",
    "        Train_Current_State=np.random.randint(0,self.State_Number-1)\n",
    "        self.Epsilon = self.Epsilon_Begin\n",
    "        while True:\n",
    "            Train_Action=self.Select_Action(Train_Current_State)\n",
    "            #print(\"TA\",Train_Action)\n",
    "            Train_Next_State,Train_Reward,Train_Done=self.Next_State(Train_Action,Train_Current_State)\n",
    "            self.Save_Memory(Train_Current_State,Train_Action,Train_Next_State,Train_Reward,Train_Done)\n",
    "            if self.Step>self.Observe:\n",
    "                self.Experience_Replay()\n",
    "            if self.Step>200000:\n",
    "                 break;\n",
    "            if Train_Done:\n",
    "                Train_Current_State=np.random.randint(0,self.State_Number-1)\n",
    "            else:\n",
    "                Train_Current_State=Train_Next_State\n",
    "                self.Step+=1          \n",
    "    #***************************************************************\n",
    "    def Play(self):\n",
    "        self.Train()\n",
    "        Start_Room_Index=0;\n",
    "        Play_Current_State_Index=0\n",
    "        Play_Step=0;\n",
    "        print(\"****************** Agent is in the State\",Start_Room_Index,\"**********************\")\n",
    "        while(Play_Current_State_Index!=self.State_Number-1):\n",
    "            Play_Current_State=self.State_List[Play_Current_State_Index:Play_Current_State_Index+1]\n",
    "            Play_Action=(self.session.run(self.Predict,feed_dict={self.State_Input:Play_Current_State}))\n",
    "            if Play_Action==0:\n",
    "                Play_Current_State_Index=Play_Current_State_Index-1\n",
    "            elif Play_Action==1:\n",
    "                Play_Current_State_Index=Play_Current_State_Index+1\n",
    "            elif Play_Action==2:\n",
    "                Play_Current_State_Index=Play_Current_State_Index+self.N_Mesh\n",
    "            else:\n",
    "                Play_Current_State_Index=Play_Current_State_Index-self.N_Mesh\n",
    "            Play_Step+=1\n",
    "            print(\"Step is \",Play_Step,\"****************** Agent is in the State\", Play_Current_State_Index,\"**********************\")\n",
    "            if(Play_Step>30):\n",
    "                break\n",
    "        if Play_Current_State_Index==self.State_Number-1:\n",
    "            self.Graph()\n",
    "    #***************************************************************        \n",
    "    def Caculate_coordinate(self,State):\n",
    "        X_Axis=0.5+(State-1)%self.N_Mesh\n",
    "        Y_Axis=0.5+((State-1)//self.N_Mesh)\n",
    "        return X_Axis,Y_Axis \n",
    "    #***************************************************************  \n",
    "    def Graph(self): \n",
    "        Graph_Step=0\n",
    "        fig=plt.figure()\n",
    "        ax=fig.gca()\n",
    "        ax.set(xlim=[0, self.N_Mesh], ylim=[0, self.N_Mesh])\n",
    "        ax.set_xticks(np.arange(0,(self.N_Mesh+1)))\n",
    "        ax.set_yticks(np.arange(0,(self.N_Mesh+1)))\n",
    "        plt.grid()\n",
    "        Graph_Start_Index=0\n",
    "        Graph_Current_Index=Graph_Start_Index\n",
    "        while(Graph_Current_Index!=self.State_Number-1):\n",
    "            Graph_Current_State=self.State_List[Graph_Current_Index:Graph_Current_Index+1]\n",
    "            Graph_Action=(self.session.run(self.Predict,feed_dict={self.State_Input:Graph_Current_State}))\n",
    "            if(Graph_Action==0):\n",
    "                Graph_Next_Index=Graph_Current_Index-1\n",
    "            elif(Graph_Action==1):\n",
    "                Graph_Next_Index=Graph_Current_Index+1\n",
    "            elif(Graph_Action==2):\n",
    "                 Graph_Next_Index=Graph_Current_Index+self.N_Mesh\n",
    "            else:\n",
    "                 Graph_Next_Index=Graph_Current_Index-self.N_Mesh\n",
    "            Graph_Step+=1\n",
    "            X_State,Y_State=self.Caculate_coordinate(Graph_Current_Index+1)\n",
    "            X_Next_State,Y_Next_State=self.Caculate_coordinate(Graph_Next_Index+1)\n",
    "            if(Graph_Step==0):\n",
    "                plt.scatter(X_State,Y_State)  \n",
    "            else:\n",
    "                plt.scatter(X_State,Y_State) \n",
    "                plt.scatter(X_Next_State,Y_Next_State)\n",
    "                plt.plot([X_State,X_Next_State],[Y_State,Y_Next_State])\n",
    "            for J in np.arange(len(self.Obstacle)):\n",
    "                plt.scatter((self.Caculate_coordinate(self.Obstacle[J]+1))[0],(self.Caculate_coordinate(self.Obstacle[J]+1))[1],marker=\"x\")\n",
    "            Graph_Current_Index=Graph_Next_Index\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    q_network = Q_Network()\n",
    "    q_network.Play()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
